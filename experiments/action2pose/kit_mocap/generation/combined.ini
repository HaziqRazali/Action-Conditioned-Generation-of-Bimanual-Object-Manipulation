
# train
# - batch_size			= 32
# - teacher_force_ratio	= 0.6
# test
# - batch_size			= 2
# - teacher_force_ratio	= 0.0

# architecture
architecture = "models.action2pose.graph_v1"

# log and weights will be stored at log_root/experiment_name and weight_root/experiment_name respectively
# experiment_name != result_name so I can store the results given various settings
log_root		= "~/Action-Conditioned-Generation-of-Bimanual-Object-Manipulation/logs/action2pose"
weight_root 	= "~/Action-Conditioned-Generation-of-Bimanual-Object-Manipulation/weights/action2pose"
experiment_name	= "kit_mocap/generation/combined"

# dataset will be loaded from data_root/data_folder
data_loader			= "dataloaders.action2pose.kit_mocap"
data_root			= "~/Action-Conditioned-Generation-of-Bimanual-Object-Manipulation/datasets/kit_mocap"
data_name			= "data"
# in case i want to manually provide a path to the cached data
cached_data_path	= None

# dataset settings
main_actions 			= ["Open", "Close", "Cut", "Mix", "Peel", "Pour", "RollOut", "Scoop", "Stir", "Transfer", "Wipe"]
main_actions_scale		= []
sample_ratio			= [3, 3, 5.35, 8.91, 8.91, 1.3, 9.72, 1, 1.7, 1.49, 1.99]
fine_actions			= ["Approach", "Move", "Hold", "Place", "Retreat", "Idle"]
resolution				= 0.02 
inp_length				= 0
out_length				= 0
time_step_size  		= 0.10
xyz_scale				= 0.1
kin_scale				= 1
add_distractors			= 0
num_extra_distractors	= -1
object_padded_length 	= 4
pose_padded_length		= 130
train_samples			= [0,1]
val_samples				= [2]
sample_from				= "full_sequence"
sample_till				= "full_sequence"
add_noise				= 1 
sigma					= 1
noise_scale				= 0
noise_add_type			= "all_objects"
noise_probability		= 0.0

# optimization
batch_size			= 32
seed			 	= 1337
lr					= 1e-3
tr_step				= 100
va_step				= 100
teacher_force_ratio	= 0.6
loss_names			= ["obj_xyz", "wrist_xyz", "obj_distribution", "obj_xyz_vel", "wrist_xyz_vel", "xyz", "finger", "lqk", "rqk"]
loss_functions		= ["mse", "mse", "nonvanilla_kl_loss", "mse", "mse", "mse", "mse", "cross_entropy", "cross_entropy"]
loss_weights		= ["[1.0]*1000", "[1.0]*1000", "[1e-3]*1000", "[1.0]*1000", "[1.0]*1000", "[1.0]*1000", "[1.0]*1000", "[1.0]*1000", "[1.0]*1000"]
task_names			= ["all"]
task_components		= [["obj_xyz", "wrist_xyz", "obj_distribution", "obj_xyz_vel", "wrist_xyz_vel", "xyz", "finger", "lqk", "rqk"]]
freeze				= None
reset_loss			= None

# architecture details
# hand_decoder_units[0] = pose_decoder_units[-1] + num_obj_classes
hand_xyz_dims				= [13, 14, 15, 25, 26, 37, 38, 39, 49, 50]
hand_encoder_type			= "make_mlp" 
hand_encoder_units			= [15, 64] 
hand_encoder_activations	= ["none"]
hand_decoder_units			= [88, 15] 
hand_decoder_activations	= ["none"]

# object_decoder_units[0] = pose_decoder_units[-1] + num_obj_classes
object_encoder_units		= [12, 64]
object_encoder_activations	= ["none"]
object_decoder_units		= [88, 12]
object_decoder_activations	= ["none"]

# pose_encoder_units[0] = (object_encoder_units + num_obj_classes) * 2
# pose_rnn_encoder_units[0] = pose_encoder_units[-1] + 1
pose_encoder_units			= [176, 64, 64] 
pose_encoder_activations	= ["relu", "relu"]
pose_rnn_encoder_units		= [76, 128, 2, 76, 128] 
pose_rnn_encoder_activations= ["none"]
pose_mu_var_units			= [128, 4] 
pose_mu_var_activations		= ["none"]

# pose_rnn_decoder_units[0] = pose_encoder_units[-1] + 1 + z
# pose_decoder_units[0] = pose_rnn_decoder_hidden * num_layers * 2
pose_rnn_decoder_units		= [80, 128, 2, 80, 128] 
pose_rnn_decoder_activations= ["none"]
pose_decoder_units			= [256, 64] 
pose_decoder_activations	= ["relu"]

# body decoder
body_decoder_type			= "rnn"
body_decoder_units			= [159, 128, 2, 159, 128] 
body_decoder_activations	= ["none"]

# finger_decoder_units[0] = pose_decoder_units[-1] + num_obj_classes
finger_decoder_type			= "rnn"
finger_decoder_units		= [75, 128, 2, 75, 128] 
finger_decoder_activations	= ["none"]
use_edges_for_finger		= "attention"

# other architecture details
object_type					= "3d"
predict_object_velocity		= 1
predict_body_velocity		= 1
predict_finger_velocity		= 0
use_main_action				= 1
use_semantic_variation		= 0
use_object_label			= 1
position_embedder_type		= "progress_ratio_position_embedder"
reverse_position_embedder	= 0
position_embedder_units		= 1
position_fusion				= "cat"

# checkpointing
restore_from_checkpoint	= 0
epoch_names		= [-1]
layer_names		= [["hand_encoder", "hand_decoder", "object_encoder", "object_decoder", "prior_net", "posterior_net", "decoder_net", "body_decoder", "finger_decoder", "hand_encoder2", "object_encoder2", "object_embedder", "action_embedder"]]
strict			= 1
remove			= ["prior","posterior","distribution"]

# json results will be stored in result_root/result_name
# experiment_name != result_name so I can store the results given various settings
result_root		= "~/Action-Conditioned-Generation-of-Bimanual-Object-Manipulation/results/action2pose" 
result_name		= "kit_mocap/generation/combined"
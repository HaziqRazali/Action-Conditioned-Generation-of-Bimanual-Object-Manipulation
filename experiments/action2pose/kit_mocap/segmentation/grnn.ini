
# architecture
architecture = "models.action2pose.grnn"

# log and weights will be stored at log_root/experiment_name and weight_root/experiment_name respectively
# experiment_name != result_name so I can store the results given various settings
log_root		= "~/Action-Conditioned-Generation-of-Bimanual-Object-Manipulation/logs/action2pose"
weight_root 	= "~/Action-Conditioned-Generation-of-Bimanual-Object-Manipulation/weights/action2pose"
experiment_name	= "kit_mocap/segmentation/grnn"

# dataset will be loaded from data_root/data_folder
data_loader			= "dataloaders.action2pose.kit_mocap"
data_root			= "~/datasets/kit_mocap"
data_name			= "data-sorted-simpleGT-v4-xml-only"
# in case i want to manually provide a path to the cached data
cached_data_path	= None

# dataset settings
main_actions 			= ["Cut", "Mix", "Peel", "Pour", "RollOut", "Scoop", "Stir", "Transfer", "Wipe"]
main_actions_scale		= []
sample_ratio			= [5.35, 8.91, 8.91, 1.3, 9.72, 1, 1.7, 1.49, 1.99]
fine_actions			= ["Approach", "Move", "Hold", "Place", "Retreat", "Idle"]
resolution				= 0.02 
inp_length				= 0
out_length				= 0
time_step_size  		= 0.20
xyz_scale				= 0.1
kin_scale				= 1
add_distractors			= 1
num_extra_distractors	= -1
object_padded_length 	= 8
pose_padded_length		= 80
train_samples			= [0,1]
val_samples				= [2]
sample_from				= "full_sequence"
sample_till				= "full_sequence"
add_noise				= 1 
sigma					= 1
noise_scale				= 2 
noise_add_type			= "all_objects"
noise_probability		= 0.5

# optimization
batch_size			= 32
seed			 	= 1337
lr					= 1e-3
tr_step				= 100
va_step				= 100
loss_names			= ["lhand_action_ids", "rhand_action_ids"]
loss_functions		= ["padded_cross_entropy", "padded_cross_entropy"]
loss_weights		= ["[1.0]*1000", "[1.0]*1000"]
task_names			= ["all"]
task_components		= [["lhand_action_ids", "rhand_action_ids"]]
freeze				= None
reset_loss			= None

# architecture details
hand_xyz_dims					= [13, 14, 15, 25, 26, 37, 38, 39, 49, 50]
hand_encoder_type				= "make_mlp" 
hand_encoder_units				= [15, 64] 
hand_encoder_activations		= ["none"]
hand_decoder_units				= [70, 15] 
hand_decoder_activations		= ["none"]

# object encoder
object_encoder_units 			= [12, 64]
object_encoder_activations		= ["none"]

# pose encoder
pose_encoder_units				= [172, 128]
pose_encoder_activations		= ["relu"]
pose_rnn_encoder_units			= [150, 256, 2, 150, 256]
pose_rnn_encoder_activations	= ["relu"]
use_bidirectional				= 1

# other architecture details
object_type						= "3d"
context							= 1
tanh_before_clf					= 1

# checkpointing
restore_from_checkpoint	= 0
epoch_names		= [-1]
layer_names		= [["hand_encoder", "object_encoder", "edge_conv", "pose_encoder", "pose_encoder_hi", "lclf", "rclf"]]
strict			= 1
remove			= ["prior","posterior","distribution"]

# json results will be stored in result_root/result_name
# experiment_name != result_name so I can store the results given various settings
result_root		= "~/Action-Conditioned-Generation-of-Bimanual-Object-Manipulation/results/action2pose" 
result_name		= "kit_mocap/segmentation/grnn"